<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">First Steps with Dapr</title><link rel="alternate" href="http://www.ofbizian.com/2022/08/first-steps-with-dapr.html" /><author><name>Unknown</name></author><id>http://www.ofbizian.com/2022/08/first-steps-with-dapr.html</id><updated>2022-08-13T10:33:00Z</updated><content type="html">I recently to and work on the Dapr project. I about Dapr when it was initially announced by Microsoft, but hadn’t looked into it since it CNCF. Two years later, during my onboarding into the new role, I spent some time looking into it and here are the steps I took in the journey and my impressions so far. WHAT IS DAPR? TL;DR: Dapr is a distributed systems toolkit in a box. It addresses the peripheral integration concerns of applications and lets developers focus on the business logic. If you are familiar with Apache Camel, Spring Framework in the Java world, or other distributed systems frameworks, you will find a lot of similarities with Dapr. Here are a few parallels with other frameworks: * Similar to Camel, Dapr has connectors (called ) that let you connect to various external systems. * Similar to HashiCorp Consul, Dapr offers which can be backed by Consul. * Similar to Spring Integration, Spring Cloud, (remember Netflix Hystrix?) and many other frameworks, Dapr has error handling capabilities with retries, timeouts, circuit breakers which are called . * Similar to Spring Data KeyValue, Dapr offers Key/Value-based state abstractions. * Similar to Kafka, Dapr offers pub/sub-based service interactions. * Similar to ActiveMQ clients, Dapr offers , but these are not specific to a messaging technology, which means they can be used even with things such as AWS SQS or Redis for example. * Similar to Spring Cloud Config, Dapr offers configuration and secret management * Similar to Zookeeper or Redis clients, Dapr offers * Similar to a Service Mesh, Dapr offers mTLS and between your application and the sidecar. * Similar to Envoy, Dapr offers enhanced through automatic metrics, tracing and log collection. The primary difference between all of these frameworks and Dapr is that the latter offers its capabilities not as a library within your application, but as a sidecar running next to your application. These capabilities are exposed behind well-defined HTTP and gRPC APIs (very creatively called ) where the implementations (called ) can be swapped w/o affecting your application code. High-level Dapr architecture You could say, Dapr is a collection of stable APIs exposed through a sidecar and swappable implementations running somewhere else. It is the cloudnative incarnation of integration technologies that makes integration capabilities previously available only in a few languages, available to everybody, and portable everywhere: Kubernetes, on-premise, or literally (I mean the edge). GETTING STARTED The project is surprisingly easy to get up and running regardless of your developer background and language of choice. I was able to follow the getting started guides and run various quickstarts in no time on my MacOS. Here are roughly the steps I followed. INSTALL DAPR CLI Dapr CLI is the main tool for performing Dapr-related tasks such as running an application with Dapr, seeing the logs, running Dapr dashboard, or deploying all to Kubernetes. brew install dapr/tap/dapr-cli With the CLI installed, we have a few different options for installing and running Dapr. I’ll start from the least demanding and flexible option and progress from there. OPTION 1: INSTALL DAPR WITHOUT DOCKER This is the lightest but not the most useful way to run Dapr. dapr init --slim In this only daprd and placement binaries are installed on the machine which is sufficient for running Dapr sidecars locally. Run a Dapr sidecar The following command will start a Dapr sidecar called no-app listening on HTTP port 3500 and a random gRPC port. dapr run --app-id no-app --dapr-http-port 3500 Congratulations, you have your first Dapr sidecar running. You can see the sidecar instance through this command: dapr list or query its health status: curl -i http://localhost:3500/v1.0/healthz Dapr sidecars are supposed to run next to an application and not on their own. Let’s stop this instance and run it with an application. dapr stop --app-id no-app Run a simple app with a Dapr sidecar For this demonstration we will use a simple NodeJS : git clone cd samples/hello-dapr-slim npm install This is a Hello World the Dapr way and here is the gist of it: app.post('/neworder', bodyParser.json(), (req, res) =&gt; { const data = req.body.data; const orderId = data.orderId; res.status(200).send("Got a new order! Order ID: " + orderId); }); The application has one /neworder endpoint listening on port 3000. We can run this application and the sidecar with the following command: dapr run --app-id nodeapp --app-port 3000 --dapr-http-port 3500 node app.js The command starts the NodeJS application on port 3000 and Dapr HTTP endpoint on 3500. Once you see in the logs that the app has started successfully, we can poke it. But rather than hitting the /neworder endpoint directly on port 3000, we will instead interact with the application through the sidecar. We do that using Dapr CLI like this: dapr invoke --verb POST --app-id nodeapp --method neworder --data '{"data": { "orderId": "41" } }' And see the response from the app. If you noticed, the CLI only needs the app-id (instead of host and port) to locate where the service is running. The CLI is just a handy way to interact with your service. It that seems like too much magic, we can use bare-bones curl command too: curl -XPOST -d @sample.json -H "Content-Type:application/json" http://localhost:3500/v1.0/invoke/nodeapp/method/neworder This command uses the service Dapr’s invocation API to synchronously interact with the application. Here is a visual representation of what just happened: Invoking an endpoint through Dapr sidecar Now, with Dapr on the request path, we get the Daprized service invocation benefits such as resiliency policies such as retries, timeouts, circuit breakers, control; observability enhancements such as: metrics, tracing, logs; security enhancements such as mTLS, , etc. At this point, you can try out metadata, metrics endpoints, play with the options, or see your single microservice in Dapr dashboard. dapr dashboard The slim mode we are running on is good for the Hello World scenario, but not the best setup for local development purposes as it lacks state store, pub/sub, metric server, etc. Let’s stop the nodeapp using the command from earlier (or CTL +C), and remove the slim Dapr binary: dapr uninstall One thing to keep in mind is that this command will not remove the default configuration and component specification files usually located in: ~/.dapr folder. We didn’t create any files in the steps so far, but if you follow other tutorials and change those files, they will remain and get applied with every dapr run command in the future (unless overridden). This caused me some confusion, keep it in mind. OPTION 2: INSTALL DAPR WITH DOCKER This is the preferred way for running for development purposes but it requires Docker. Let’s set it up: dapr init The command will download and run 3 containers * Dapr placement container used with actors(I wish this was an optional feature) * Zipkin for collecting tracing information from our sidecars * And a single node Redis container used for state store, pub/sub, distributed-lock implementations. You can verify when these containers are running and you are ready to go. docker ps RUN THE QUICKSTARTS My next step from here was to try out the that demonstrate the building blocks for service invocation, pub/sub, state store, bindings, etc. The awesome thing about these quickstarts is that they demonstrate the same example in multiple ways: * With Dapr SDK and w/o any dependency to Dapr SDK i.e. using HTTP only. * In multiple languages: Java, Javascript, .Net, Go, Python, etc. You can mix and match different languages and interaction methods (SDK or native) for the same example which demonstrates Dapr’s polyglot nature. Option 3: Install Dapr on Kubernetes If you have come this far, you should have a good high-level understanding of what Dapr can do for you. The next step would be to deploy where most of the Dapr functionalities are available and closest to a production deployment. For this purpose, I used minikube locally with default settings and no custom tuning. dapr init --kubernetes --wait If successful, this command will start the following pods in dapr-system namespace: * dapr-operator: manages all components for state store, pub/sub, configuration, etc * dapr-sidecar-injector: injects dapr sidecars into deployment pods * dapr-placement: required with actors only. * dapr-sentry: manages mTLS between services and acts as a certificate authority. * dapr-dashboard: a simple webapp to explore what is running within a Dapr cluster These Pods collectively represent the Dapr . Injecting a sidecar From here on, adding a Dapr sidecar to an application (this would be Dapr dataplane) is as easy as adding the following to your Kubernetes Deployments:  annotations:    dapr.io/enabled: "true"    dapr.io/app-id: "nodeapp"    dapr.io/app-port: "3000" The dapr-sidecar-injector service watches for new Pods with the dapr.io/enabled annotation and injects a container with the daprd process within the pod. It also adds DAPR_HTTP_PORT and DAPR_GRPC_PORT environment variables to your container so that it can easily communicate with Dapr without hard-coding Dapr port values. To deploy a complete application on Kubernetes I suggest this step-by-step . It has a provider and consumer services and it worked the first time for me. TRANSPARENT VS EXPLICIT PROXY Notice Dapr sidecar injection is less intrusive than a typical service mesh with a transparent sidecar such as Istio’s Envoy. To inject a transparent proxy, typically the Pods also get injected with an init-container that runs at the start of the Pod and re-configures the Pods networking rules so that all ingress and egress traffic or your application container goes through the sidecar. With Dapr, that is not the case. There is a sidecar injected, but your application is in control of when and how to interact with Dapr over its well-defined explicit (non-transparent) APIs. Transparent service mesh proxies operate at lower network layers typically used by operations teams, whereas Dapr provides application layer primitives needed by developers. If you are interested in this topic, is a good explanation of the differences and overlaps of Dapr with services meshes. SUMMARY And finally, here are some closing thoughts with what I so far liked more and what less from Dapr. LIKED MORE * I love the fact that Dapr is one of the few CNCF projects targeting developers creating applications, and not only operations team who are running these applications. We need more cloudnative tools for implementing applications. * I love the non-intrusive nature of Dapr where capabilities are exposed over clear APIs and not through some black magic. I prefer transparent actions for instrumentation, observability, and general application insight, but not for altering application behavior. * I loved the polyglot nature of Dapr offering its capabilities to all programming languages and runtimes. This is what attracted me to Kubernetes and cloudnative in the first place. * I loved how easy it is to get started with Dapr and the many permutations of each quickstart. There is something for everyone regardless of where you are coming from into Dapr. * I’m excited about WASM and remote components features coming into Dapr. These will open new surface areas for more contributions and integrations. LIKED LESS * I haven’t used actors before and it feels odd to have a specific programming model included in a generic distributed systems toolkit. Luckily you don’t have to use it if you don’t want to. * The documentation is organized, but too sparse into multiple short pages. Learning a topic will require navigating a lot of pages multiple times, and it is still hard to find what you are looking for. Follow me at to join my journey of learning and using Dapr and shout out with any thoughts and comments.</content><dc:creator>Unknown</dc:creator></entry><entry><title>Implement multitenant SaaS on Kubernetes</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/12/implement-multitenant-saas-kubernetes" /><author><name>Bob Reselman</name></author><id>5072fe6d-3baf-4cac-80c4-b119a34825b1</id><updated>2022-08-12T07:00:00Z</updated><published>2022-08-12T07:00:00Z</published><summary type="html">&lt;p&gt;This article is the second in a series about implementing a multitenant, &lt;a href="/topics/containers"&gt;containerized&lt;/a&gt; SaaS application. The first article, &lt;a href="/articles/2022/06/16/how-convert-web-application-software-service"&gt;How to convert a web application to Software-as-a-Service&lt;/a&gt;, discussed from a conceptual point of view how to convert a standalone web application into generic code that powers a SaaS platform. This article demonstrates in a concrete manner how to implement a multitenant SaaS in a &lt;a href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; cluster.&lt;/p&gt; &lt;p&gt;The example in the previous article converted a fictitious standalone application named Clyde's Clarinets into a SaaS platform named Instrument Resellers. The purpose of Clyde's Clarinets was to acquire, refurbish, and resell used clarinets. Clyde's Clarinets evolved into the Instrument Resellers SaaS platform so that any business could acquire, refurbish, and resell a particular type of instrument. Thus, Instrument Resellers has the capability to support tenants such as Betty's Brass and Sidney's Saxophones as well as Clyde's Clarinets. (See Figure 1.)&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/transform.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/transform.png?itok=_hc5v4A3" width="1015" height="331" alt="A standalone web application was transformed into a SaaS platform." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A standalone web application was transformed into a SaaS platform. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Implementing SaaS on Kubernetes&lt;/h2&gt; &lt;p&gt;This article describes how to use standard Kubernetes resources—namespaces, deployments, and services—to create different tenants using a common code base. In addition to the standard Kubernetes resources, we use the &lt;a href="https://docs.openshift.com/container-platform/4.10/networking/routes/route-configuration.html"&gt;route resource&lt;/a&gt; provided by &lt;a href="/openshift"&gt;Red Hat OpenShift&lt;/a&gt; to create a public URL that enables access to the internal Kubernetes service representing the particular tenant application instance.&lt;/p&gt; &lt;p&gt;The demonstration code runs on the Red Hat OpenShift Container Platform because its route resource provides an easy way to create a domain name that provides access to a tenant running within the Kubernetes cluster.&lt;/p&gt; &lt;p&gt;This article refers to demonstration code for implementing the Instrument Reseller SaaS platform. A subsequent article in this series will describe the code in the demonstration project in detail. For now, you can use the demonstration project as a supporting reference for this article.&lt;/p&gt; &lt;p&gt;Be advised that in order to get the full benefit from reading this article, you need to have an understanding of containers and Kubernetes, particularly around the purpose and use of Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/"&gt;pods&lt;/a&gt;, &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;deployments&lt;/a&gt;, &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Secrets&lt;/a&gt;, and &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service/"&gt;services&lt;/a&gt;. Also, you need to have experience working with the &lt;a href="https://kubernetes.io/docs/reference/kubectl/"&gt;kubectl&lt;/a&gt; client for Kubernetes. In addition, you should be comfortable creating Kubernetes resources using &lt;a href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/"&gt;manifest (a.k.a. configuration) files&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The following sections describe how to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use Kubernetes namespaces to isolate tenants in a SaaS platform.&lt;/li&gt; &lt;li&gt;Configure Kubernetes deployments to dedicate application logic to a specific tenant.&lt;/li&gt; &lt;li&gt;Bind a database to a particular tenant using a Kubernetes Secret.&lt;/li&gt; &lt;li&gt;Present a tenant's application logic to the internal network in the cluster using a Kubernetes service.&lt;/li&gt; &lt;li&gt;Expose the tenant outside of the cluster using an OpenShift route.&lt;/li&gt; &lt;li&gt;Deploy and update tenant application logic using a basic &lt;a href="/topics/ci-cd"&gt;continuous integration/continuous deployment (CI/CD)&lt;/a&gt; process.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The role of Kubernetes namespaces in SaaS&lt;/h2&gt; &lt;p&gt;Supporting multiple tenants in a single cluster has been a fundamental feature in Kubernetes since its initial release. Under Kubernetes, it is entirely possible for many tenants to share instances of a common code base while running in isolation from each other.&lt;/p&gt; &lt;p&gt;There are &lt;a href="/articles/2022/05/09/approaches-implementing-multi-tenancy-saas-applications"&gt;several possible approaches&lt;/a&gt; to multitenancy in a SaaS platform under Kubernetes:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Build tenant isolation right into the logic of a single application.&lt;/li&gt; &lt;li&gt;Run each tenant in its own cluster.&lt;/li&gt; &lt;li&gt;Run each tenant in its own Kubernetes namespace.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;The Instrument Reseller SaaS platform takes the third approach and uses namespaces to support multiple tenants in a single Kubernetes cluster. This section explains the details of the namespace concept.&lt;/p&gt; &lt;p&gt;Namespaces, as the name implies, create an operational boundary that can be imposed on other resources. For example, you can create a namespace named &lt;code&gt;foo&lt;/code&gt;, and then create other resources such as pods and services under that &lt;code&gt;foo&lt;/code&gt; namespace. Those resources know only about other resources in the &lt;code&gt;foo&lt;/code&gt; namespace. Resources outside of that namespace have no access to resources inside the namespace.&lt;/p&gt; &lt;p&gt;In a multitenant service using namespace isolation, each tenant in the Kubernetes cluster is represented by a particular namespace. The deployment, service, and route resources for the particular tenant are created in that tenant's namespace. Figure 2 illustrates how Kubernetes namespaces isolate tenants in the Instrument Resellers SaaS platform.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/tenants.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/tenants.png?itok=8tDzGWKq" width="385" height="565" alt="Each tenant has its own namespace and URL, but runs the same application." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Each tenant has its own namespace and URL, but runs the same application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Although three tenants are shown in Figure 2, this article shows configurations just for Betty's Brass and Clyde's Clarinets because two tenants are enough to illustrate the concepts you need to know. Table 1 shows the manifest files that declare the Kubernetes namespaces for these tenants. The two manifests are the same except for the &lt;code&gt;name&lt;/code&gt; properties.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 1: Manifests declaring namespaces.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;kind: Namespace apiVersion: v1 metadata: name: bettysbrass labels: name: bettysbrass&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;kind: Namespace apiVersion: v1 metadata: name: clydesclarinets labels: name: clydesclarinets&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;To create each namespace in the Kubernetes cluster, run the following command, where &lt;code&gt;&amp;lt;tenant_namespace&amp;gt;&lt;/code&gt; is the filename of the manifest file particular to the tenant:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f &amp;lt;tenant_namespace&amp;gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Creating and configuring tenants within a SaaS using a Kubernetes deployment&lt;/h2&gt; &lt;p&gt;Once the namespaces are created, the next task is to implement the logic for the given tenant according to its assigned namespace. This task uses the Kubernetes deployment resource.&lt;/p&gt; &lt;p&gt;As mentioned previously, a key feature of the Instrument Reseller SaaS is that a single code base can support any number of tenants that want to acquire and resell musical instruments. Application logic for each instrument reseller is represented in the SaaS by a Kubernetes deployment resource.&lt;/p&gt; &lt;p&gt;A deployment controls one or many pod &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"&gt;replicas&lt;/a&gt;. The number of pods running under a deployment is determined by the &lt;code&gt;replicas&lt;/code&gt; property in the deployment resource's manifest file.&lt;/p&gt; &lt;p&gt;Therefore, you can change the number of replicas a deployment supports while the application is running. For example, an instrument reseller might start by running three pods. But, after a while, the load on the tenant is such that more pods are needed. To create more pods in the deployment, increase the value assigned to the &lt;code&gt;replicas&lt;/code&gt; property in the manifest file—from three to five, for example. Then re-apply the deployment's manifest file to the cluster. When loads decrease, you can reduce the number of pods in the deployment by changing the &lt;code&gt;replicas&lt;/code&gt; setting in the manifest file and reapplying the changed file to the cluster in the same way.&lt;/p&gt; &lt;p&gt;Should a pod go offline, the deployment resource will create a replacement if possible.&lt;/p&gt; &lt;h3&gt;Customizing deployments&lt;/h3&gt; &lt;p&gt;In our architecture, each deployment should be dedicated to a single instrument reseller. You create the deployment in that instrument reseller's namespace and define the parameters needed by that reseller, such as the URL where it takes orders, through environment variables in the Kubernetes manifest.&lt;/p&gt; &lt;p&gt;For instance, Table 2 shows the manifests that configure the Kubernetes deployment for Betty's Brass and Clyde's Clarinets. The only differences are the values for names and instruments.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 2: Manifests configuring deployments.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: instrumentreseller namespace: bettysbrass labels: app: instrumentreseller spec: replicas: 3 selector: matchLabels: app: instrumentreseller template: metadata: labels: app: instrumentreseller spec: initContainers: - name: seeder image: quay.io/rhdevelopers/instrumentresellerseeder env: - name: RESELLER_DB_NAME value: "brass" - name: RESELLER_INSTRUMENT value: "brass" - name: SEEDER_COUNT value: "10" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url containers: - name: instrumentreseller image: quay.io/rhdevelopers/instrumentreseller env: - name: RESELLER_NAME value: "Betty's Brass" - name: RESELLER_INSTRUMENT value: "brass" - name: RESELLER_DB_NAME value: "brass" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url ports: - containerPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: apps/v1 kind: Deployment metadata: name: instrumentreseller namespace: clydesclarinets labels: app: instrumentreseller spec: replicas: 3 selector: matchLabels: app: instrumentreseller template: metadata: labels: app: instrumentreseller spec: initContainers: - name: seeder image: quay.io/rhdevelopers/instrumentresellerseeder env: - name: RESELLER_DB_NAME value: "clarinets" - name: SEEDER_COUNT value: "10" - name: RESELLER_INSTRUMENT value: "clarinet" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url containers: - name: instrumentreseller image: quay.io/rhdevelopers/instrumentreseller env: - name: RESELLER_NAME value: "Clyde's Clarinets" - name: RESELLER_INSTRUMENT value: "clarinet" - name: RESELLER_DB_NAME value: "clarinets" - name: MONGODB_URL valueFrom: secretKeyRef: name: mongo-url key: url ports: - containerPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;A key point to understand about the previous examples is that both tenants are using the same container images. In every tenant, the init container uses the &lt;code&gt;quay.io/rhdevelopers/instrumentresellerseeder&lt;/code&gt; image and the main container uses the &lt;code&gt;quay.io/rhdevelopers/instrumentreseller&lt;/code&gt; image. Remember, an essential principle of multiple tenancy in a SaaS platform is that all tenants use the same code base. Having multiple tenants use the same container images supports this basic principle.&lt;/p&gt; &lt;p&gt;Each tenant in the SaaS platform binds to its own database. That database might exist within the Kubernetes cluster or be an external database service defined by a URL. Often, username and password information needed to access the database will be part of the URL.&lt;/p&gt; &lt;p&gt;Putting username and password information in a cluster is always a risky undertaking. A best practice for making username/password information available to pods in a Kubernetes cluster is to use a Kubernetes resource called a &lt;em&gt;Secret&lt;/em&gt;. We will see how our application passes credentials to the database shortly.&lt;/p&gt; &lt;h3&gt;Data seeding&lt;/h3&gt; &lt;p&gt;As briefly mentioned earlier, the pods in the deployment use &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/"&gt;init containers&lt;/a&gt; in addition to standard containers. An init container is a container that runs before the main container. In the case of the Instrument Reseller SaaS, the init container does the work of implementing a special feature of the demonstration code: &lt;em&gt;data seeding.&lt;/em&gt;&lt;/p&gt; &lt;p&gt;Because we're not working with real retailers in the demo, we use the init container to seed the tenant instance's database with randomized data that is particular to the instrument type sold by the instrument reseller. The purpose of data seeding in the demo is to provide some initial data to view when the application is used for the first time. Betty's Brass will be seeded with data about brass instruments. Clyde's Clarinets will be seeded with data about clarinets. Sidney's Saxophones will be seeded with data specific to saxophones.&lt;/p&gt; &lt;p&gt;Using the data seeding pattern in containers to prepopulate data for an application opens up the risk of redundant seeding. If one simply runs the init container in each pod replica, the deployment tries to seed data to the data source when each replica starts. Unless a precaution is made, unwarranted data seeding will occur.&lt;/p&gt; &lt;p&gt;Therefore, the data seeder is programmed to go out to the data source and check whether pre-existing seed data exists. If seed data is already in the data source, the seeder exits without adding more data.&lt;/p&gt; &lt;h2&gt;Providing database credentials through Kubernetes Secrets&lt;/h2&gt; &lt;p&gt;Secrets are a Kubernetes resource for providing sensitive information to other resources in a secure manner.&lt;/p&gt; &lt;p&gt;Table 3 shows configurations that declare a Secret named &lt;code&gt;mongo-url&lt;/code&gt; in two different namespaces: one for Betty's Brass and the other for Clyde's Clarinets.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 3: Manifests configuring Secrets.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt; apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: bettysbrass type: Opaque stringData: url: &amp;lt;mongo-url-here&amp;gt;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt; apiVersion: v1 kind: Secret metadata: name: mongo-url namespace: clydesclarinets type: Opaque stringData: url: &amp;lt;mongo-url-here&amp;gt;&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;Note that each Secret is assigned to its respective namespace. The Secret named &lt;code&gt;mongo-url&lt;/code&gt; for Betty's Brass is assigned to the &lt;code&gt;bettysbrass&lt;/code&gt; namespace. The Secret with the same &lt;code&gt;mongo-url&lt;/code&gt; name for Clyde's Clarinets is assigned to the &lt;code&gt;clydesclarinets&lt;/code&gt; namespace. Even though each Secret has the same name, they are distinct because they are assigned to different namespaces. Using the same name among resources is one of the benefits of using namespaces.&lt;/p&gt; &lt;h2&gt;Exposing application logic using a Kubernetes service&lt;/h2&gt; &lt;p&gt;Once the Secret is configured for each tenant, the next step is to create the Kubernetes service that exposes the application logic to the internal Kubernetes network within the SaaS platform. Table 4 shows configurations for the Kubernetes service in Betty's Brass using the &lt;code&gt;bettysbrass&lt;/code&gt; namespace, and for Clyde's Clarinets using the &lt;code&gt;clydesclarinets&lt;/code&gt; namespace. Once again, assigning each service to a different namespace keeps the tenants isolated.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 4: Manifests configuring the services.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Service metadata: name: instrumentreseller namespace: bettysbrass spec: selector: app: instrumentreseller ports: - protocol: TCP port: 8088 targetPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: v1 kind: Service metadata: name: instrumentreseller namespace: clydesclarinets spec: selector: app: instrumentreseller ports: - protocol: TCP port: 8088 targetPort: 8088&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;h2&gt;Exposing the tenant outside of the cluster using an OpenShift route&lt;/h2&gt; &lt;p&gt;The last configuration step is to create the OpenShift route resource that publishes a domain name to expose the tenant outside of the Kubernetes cluster. The manifests in Table 5 declare the OpenShift routes for Betty's Brass and Clyde's Clarinets. Each manifest uses its tenant's namespace as well as a different host.&lt;/p&gt; &lt;table&gt; &lt;caption&gt;Table 5: Manifests configuring OpenShift routes.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;th&gt;Betty's Brass&lt;/th&gt; &lt;th&gt;Clyde's Clarinets&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: route.openshift.io/v1 kind: Route metadata: name: instrumentreseller namespace: bettysbrass spec: host: bettysbrass.com port: targetPort: 8088 to: kind: Service name: instrumentreseller&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;td&gt; &lt;pre&gt; &lt;code class="java"&gt;apiVersion: route.openshift.io/v1 kind: Route metadata: name: instrumentreseller namespace: clydesclarinets spec: host: clydesclarinets.com port: targetPort: 8088 to: kind: Service name: instrumentreseller&lt;/code&gt;&lt;/pre&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;p&gt;The route knows which service to bind to through the &lt;code&gt;to&lt;/code&gt; attribute at the bottom of each manifest file.&lt;/p&gt; &lt;p&gt;Declaring a set of manifest files for the Kubernetes namespace, deployment, Secret, service, and route are the first steps to getting a tenant up and running in a Kubernetes cluster. Once the manifest files are created, execute the following command to get each of the tenants running in the Kubernetes cluster, where &lt;code&gt;&amp;lt;manifest_file&amp;gt;&lt;/code&gt; is the name of the manifest file for the given tenant:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kubectl apply -f &amp;lt;manifest_file&amp;gt;.yaml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Assuming proper configuration, you'll have a tenant up and running using nothing more than a few &lt;code&gt;kubectl&lt;/code&gt; commands. However, as those of us who have spent a lot of time working with Kubernetes have come to understand, the words "proper configuration" can mean hours if not days of labor. In short, wiring everything up is hard. You have to be careful.&lt;/p&gt; &lt;p&gt;So to end this article, we'll devise a deployment process for our SaaS deployment that can be easily automated.&lt;/p&gt; &lt;h2&gt;A CI/CD release process&lt;/h2&gt; &lt;p&gt;Deploying tenants into a SaaS platform comes with varying degrees of complexity. You can do a manual deployment in which you create Linux container images for the SaaS platform's application logic and then push those images out to a container image registry such as &lt;a href="https://quay.io/"&gt;Quay.io&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Then, once the required container images are on the registry, create manifest files that you'll use to realize the Kubernetes deployment resource in the Kubernetes cluster in which the SaaS platform is running. These manifest files declare the application container images that will be used.&lt;/p&gt; &lt;p&gt;Having created the manifest files, run the &lt;code&gt;kubectl apply&lt;/code&gt; command shown near the end of the previous section to create the associated Kubernetes resource in the cluster.&lt;/p&gt; &lt;p&gt;The process just described is shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/manual.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/manual.png?itok=75GdIfGA" width="941" height="115" alt="Manual deployment supports multiple tenants, running a kubectl apply command for us." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Manual deployment supports multiple tenants, running a kubectl apply command for us. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Manual deployment is a feasible way to work with a SaaS platform for research and experimentation. But it's not realistic for today's production releases, which call for automating the process.&lt;/p&gt; &lt;p&gt;Using automation is particularly appropriate for organizations that have a number of teams supporting a SaaS platform. Relying on email and word-of-mouth communication between teams can be risky. Automation helps bring formality to the release process.&lt;/p&gt; &lt;p&gt;Central to release automation is a CI/CD controller such as &lt;a href="https://www.jenkins.io"&gt;Jenkins&lt;/a&gt; or &lt;a href="https://docs.openshift.com/container-platform/4.10/cicd/pipelines/understanding-openshift-pipelines.html"&gt;OpenShift Pipelines&lt;/a&gt;. The CI/CD controller automates many if not all of the tasks necessary to get an application's artifacts from a source code repository into production.&lt;/p&gt; &lt;p&gt;Figure 4 shows an example of a CI/CD process that updates a SaaS platform. The CI/CD controller does the work of packaging up code that's ready for release into a container image. Then it pushes that image to a container registry and updates the SaaS platform with the new version of the image.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="/sites/default/files/ci.png" data-featherlight="image"&gt;&lt;img src="/sites/default/files/styles/article_full_width_1440px_w/public/ci.png?itok=Pk4Kvmym" width="1159" height="595" alt="An automated CI/CD process for a multitenant SaaS platform uses a CI/CD controller for several steps." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: An automated CI/CD process for a multitenant SaaS platform uses a CI/CD controller for several steps. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;The numbered steps in Figure 4 are:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;The developer updates the code and commits the updated work to the dev branch in the source code repository.&lt;/li&gt; &lt;li&gt;The quality assurance (Q/A) team escalates code from the dev branch into the Q/A branch and runs unit tests. If the tests pass, Q/A executes integration testing. Upon successful execution, Q/A notifies the release management team that a new version of code is ready for escalation to the main branch of the source code repository.&lt;/li&gt; &lt;li&gt;Release management merges the code into the main branch.&lt;/li&gt; &lt;li&gt;Release management updates the Kubernetes manifest files with the new version tag of the container image associated with the intended release. Updated files are committed to the manifest file repository.&lt;/li&gt; &lt;li&gt;Upon a successful merge of source code and manifest files, the CI/CD controller is notified via automation that the code is ready for packaging into a container image and deployment to a container image registry such as Quay.io.&lt;/li&gt; &lt;li&gt;The CI/CD controller gets the updated code from the source code repository and makes an updated container image from the &lt;a href="https://github.com/containers/common/blob/main/docs/Containerfile.5.md"&gt;Containerfile&lt;/a&gt; stored in the repository along with the application source code.&lt;/li&gt; &lt;li&gt;The CI/CD controller pushes the updated container image to a container image repository.&lt;/li&gt; &lt;li&gt;The CI/CD controller gets the updated manifest files for the relevant tenants from the manifest file repository and runs the &lt;code&gt;kubectl apply&lt;/code&gt; command discussed earlier to update the pods running in the Kubernetes cluster with the container image that has the latest version of the application code.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Keep in mind that release processes usually vary among organizations. There is rarely a one-size-fits-all approach to automated releases using a CI/CD controller. This example is one of many possibilities.&lt;/p&gt; &lt;p&gt;The important thing to understand is that when an automated CI/CD process is in place, it handles much of the detailed work of getting code from a release branch into a multitenant Kubernetes cluster in production. Release tasks vary, but in general many details are handled through scripted automation in the CI/CD controller. Release personnel don't fiddle around with manual tasks unless they're facing a mission-critical emergency. Rather, changes in the CI/CD process are implemented by altering automation scripts.&lt;/p&gt; &lt;h2&gt;Kubernetes supports scalable multitenant SaaS&lt;/h2&gt; &lt;p&gt;As this article has shown, hosting a multitenant SaaS platform on Kubernetes can be straightforward. As long as the common code base used by the platform's tenants is generic, implementation involves configuring and deploying the namespace, Secret, deployment, service, and route. All these resources except the route are built into Kubernetes. The route resource is provided by OpenShift.&lt;/p&gt; &lt;p&gt;The application logic common to all tenants is encapsulated into container images that are stored in a container registry. The image is downloaded to the cluster according to configuration information set in the manifest file of the deployment for the given tenant. Finally, production-level releases are automated using a CI/CD controller.&lt;/p&gt; &lt;p&gt;Most SaaS platforms are intended for a particular set of use cases. Each tends to be special. As a result, a platform will have its own set of complexities and exceptions that need to be accommodated. Still, implementing a SaaS platform using Kubernetes is a lot easier than building one from scratch. Kubernetes does most if not all of the heavy listing.&lt;/p&gt; &lt;p&gt;This article covered the fundamental concepts and techniques for implementing a multitenant SaaS platform in a Kubernetes cluster. The next article in this series will take a detailed look at the demonstration application used in this article. That article will describe how to program the generic logic used by all tenants in the demonstration SaaS platform. The article will also describe how to get the demonstration project up and running as a multitenant SaaS platform hosted in a Kubernetes cluster.&lt;/p&gt; The post &lt;a href="/articles/2022/08/12/implement-multitenant-saas-kubernetes" title="Implement multitenant SaaS on Kubernetes"&gt;Implement multitenant SaaS on Kubernetes&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-08-12T07:00:00Z</dc:date></entry><entry><title type="html">Running Custom Tasks in jBPM With Work Item Handlers</title><link rel="alternate" href="https://blog.kie.org/2022/08/running-custom-tasks-in-jbpm-with-work-item-handlers.html" /><author><name>Helber Belmiro</name></author><id>https://blog.kie.org/2022/08/running-custom-tasks-in-jbpm-with-work-item-handlers.html</id><updated>2022-08-11T12:04:03Z</updated><content type="html">RUNNING CUSTOM TASKS IN JBPM WITH WORK ITEM HANDLERS INTRODUCTION You can use a WorkItemHandler to run custom tasks during the execution of a process in jBPM. In this article, you will run through the steps to create a custom task and use it in a process. &gt; IMPORTANT: This tutorial uses version 7.72.0.Final of jBPM. The application you’re going to create is a process that concatenates first and last names, and prints the result to the console. The concatenation is going to be processed in a custom task. So, start by creating the WorkItemHandler. &gt; NOTE: If you don’t want to create the project, you can clone it from . CREATING THE WORKITEMHANDLER 1. Run the following command to create a work item handler project: mvn archetype:generate \ -DarchetypeGroupId=org.jbpm \ -DarchetypeArtifactId=jbpm-workitems-archetype \ -DarchetypeVersion=7.72.0.Final \ -DgroupId=org.acme \ -DartifactId=myconcatworkitem \ -Dversion=1.0.0-SNAPSHOT \ -DclassPrefix=MyConcat 2. Replace the content of src/main/java/org/acme/MyConcatWorkItemHandler.java file with the following: package org.acme; import org.jbpm.process.workitem.core.AbstractLogOrThrowWorkItemHandler; import org.jbpm.process.workitem.core.util.RequiredParameterValidator; import org.jbpm.process.workitem.core.util.Wid; import org.jbpm.process.workitem.core.util.WidMavenDepends; import org.jbpm.process.workitem.core.util.WidParameter; import org.jbpm.process.workitem.core.util.WidResult; import org.jbpm.process.workitem.core.util.service.WidAction; import org.jbpm.process.workitem.core.util.service.WidAuth; import org.jbpm.process.workitem.core.util.service.WidService; import org.kie.api.runtime.process.WorkItem; import org.kie.api.runtime.process.WorkItemManager; import java.util.HashMap; import java.util.Map; @Wid(widfile = "MyConcatDefinitions.wid", name = "MyConcatDefinitions", displayName = "MyConcatDefinitions", defaultHandler = "mvel: new org.acme.MyConcatWorkItemHandler()", documentation = "myconcatworkitem/index.html", category = "myconcatworkitem", icon = "MyConcatDefinitions.png", parameters = { @WidParameter(name = "FirstName"), @WidParameter(name = "LastName") }, results = { @WidResult(name = "FullName") }, mavenDepends = { @WidMavenDepends(group = "org.acme", artifact = "myconcatworkitem", version = "1.0.0-SNAPSHOT") }, serviceInfo = @WidService(category = "myconcatworkitem", description = "${description}", keywords = "", action = @WidAction(title = "Sample Title"), authinfo = @WidAuth(required = true, params = {"FirstName", "LastName"}, paramsdescription = {"First name", "Last name"}, referencesite = "referenceSiteURL") ) ) public class MyConcatWorkItemHandler extends AbstractLogOrThrowWorkItemHandler { public void executeWorkItem(WorkItem workItem, WorkItemManager manager) { try { RequiredParameterValidator.validate(this.getClass(), workItem); String firstName = (String) workItem.getParameter("FirstName"); // Gets the "FirstName" parameter String lastName = (String) workItem.getParameter("LastName"); // Gets the "LastName" parameter String fullName = firstName + " " + lastName; // Concatenates the "firstName" and "lastName" Map results = new HashMap(); results.put("FullName", fullName); // Adds "fullName" to the "results" object manager.completeWorkItem(workItem.getId(), results); } catch (Throwable cause) { handleException(cause); } } @Override public void abortWorkItem(WorkItem workItem, WorkItemManager manager) { } } 3. Update the src/test/java/org/acme/MyConcatWorkItemHandlerTest.java test file with the following: package org.acme; import org.drools.core.process.instance.impl.WorkItemImpl; import org.jbpm.process.workitem.core.TestWorkItemManager; import org.jbpm.test.AbstractBaseTest; import org.junit.Test; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertNotNull; import static org.junit.Assert.assertTrue; public class MyConcatWorkItemHandlerTest extends AbstractBaseTest { @Test public void testHandler() { WorkItemImpl workItem = new WorkItemImpl(); workItem.setParameter("FirstName", "John"); workItem.setParameter("LastName", "Doe"); TestWorkItemManager manager = new TestWorkItemManager(); MyConcatWorkItemHandler handler = new MyConcatWorkItemHandler(); handler.setLogThrownException(true); handler.executeWorkItem(workItem, manager); assertNotNull(manager.getResults()); assertEquals(1, manager.getResults().size()); assertEquals("John Doe", manager.getResults().get(0L).get("FullName")); assertTrue(manager.getResults().containsKey(workItem.getId())); } } 4. Package and install the work item handler project into your Maven local repository. From the myconcatworkitem directory, run: mvn clean install You should see the generated myconcatworkitem-1.0.0-SNAPSHOT.jar file in the target directory. ADDING THE WORK ITEM HANDLER TO BUSINESS CENTRAL AS A CUSTOM TASK 1. Open Business Central 2. Click the gear icon in the upper-right corner 3. Click "Custom Tasks Administration" 4. Click the "Add Custom Task" button 5. Upload the myconcatworkitem-1.0.0-SNAPSHOT.jar file. After the upload, the Custom Task should appear in the list of Custom Tasks in the same window as the "Add Custom Task" button 6. Locate the Custom Task (MyConcatDefinitions) in the list and activate it INSTALLING THE CUSTOM TASK IN YOUR PROJECT 1. Open your project in Business Central and click "Settings" 2. Click "Custom Tasks" on the left corner 3. Click the "Install" button of the Custom Task (MyConcatDefinitions) 4. Click the "Save" button 5. On the left side of the screen, click "Dependencies" 6. Click "Add from Repository" and then search for the "myconcatworkitem" artifact and select it 7. Click the "Save" button and confirm the dialog USING THE CUSTOM TASK 1. Create a new Business Process 2. Add three process variables to the process. In "Properties", expand "Process Data" and add the following String variables: * firstName * lastName * fullName 3. Add a new Start Event 4. Add a new End Event 5. Click "Custom Tasks" (gear button on the left side of the screen), select "MyConcatDefinitions" and add it to the process 6. Select the node you just added and in Properties, expand "Data Assignments" and click the edit button 7. Bind the process variables to the Custom Task parameters and click OK 8. Add a Script Task to the process 9. Select the node you just added and in Properties, expand "Implementation/Execution" and add the following script: System.out.println(fullName); 10. Connect all the nodes in the process Start - MyConcatDefinitions - Task - End 11. Save and deploy the process When you start a new process instance, you’ll be asked to enter the first and last names. After submitting, you will see the concatenated fullName in the console. CONCLUSION In this tutorial, you’ve learned how to create and use a custom work item handler in jBPM by creating a process that concatenates the first and last names received as parameters. The post appeared first on .</content><dc:creator>Helber Belmiro</dc:creator></entry><entry><title>Connect to services on Kubernetes easily with kube-service-bindings</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/11/connect-services-kubernetes-easily-kube-service-bindings" /><author><name>Costas Papastathis, Michael Dawson</name></author><id>321fb836-2526-4793-b7e5-aab63bfa6520</id><updated>2022-08-11T07:00:00Z</updated><published>2022-08-11T07:00:00Z</published><summary type="html">&lt;p&gt;One of the projects the &lt;a href="/topics/nodejs"&gt;Node.js&lt;/a&gt; team at Red Hat has been focusing on over the past year is the development of &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; for &lt;a href="/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;. We've found that combining the &lt;a href="https://operatorhub.io/operator/service-binding-operator"&gt;Service Binding Operator&lt;/a&gt; and kube-service-bindings is a convenient and consistent way of sharing credentials for services, letting you easily secure your deployments.&lt;/p&gt; &lt;p&gt;This article is the first of a three-part series. Our goals in the series are to:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Introduce Kubernetes service bindings and the Service Binding Operator.&lt;/li&gt; &lt;li&gt;Explain how the kube-service-bindings NPM package supports service bindings for Node.js applications.&lt;/li&gt; &lt;li&gt;Cover the clients we've added support for in kube-service-bindings.&lt;/li&gt; &lt;li&gt;Show an end-to-end deployment of a Node.js application communicating with a database in a Kubernetes setting.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;By the end of the series, you should have a better understanding of how service bindings and kube-service-bindings work. This first article explains the tools we're using and their benefits for Node.js programmers using Kubernetes.&lt;/p&gt; &lt;h2&gt;What are service bindings and the Service Binding Operator?&lt;/h2&gt; &lt;p&gt;In our &lt;a href="https://docs.openshift.com/container-platform/4.9/applications/connecting_applications_to_services/understanding-service-binding-operator.html#service-binding-terminology"&gt;service binding terminology for Kubernetes&lt;/a&gt;, a service binding provides information about a service to a process that needs to bind to that service. In subsequent parts of this series, for instance, you will use a service binding to provide the credentials of a &lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt; database when your Node.js application connects to it. The database is called a &lt;em&gt;backing service&lt;/em&gt;, while the application is called the &lt;em&gt;workload&lt;/em&gt;. The data passed between them is called &lt;em&gt;binding data&lt;/em&gt;.&lt;/p&gt; &lt;p&gt;Protocols and rules for sharing information are laid out in the &lt;a href="https://github.com/servicebinding/spec#service-binding-specification-for-kubernetes"&gt;Service Binding Specification for Kubernetes&lt;/a&gt;. The Service Binding Operator (SBO) establishes a connection to share the credentials between the workload and backing service. The SBO is &lt;a href="https://github.com/redhat-developer/service-binding-operator"&gt;implemented by Red Hat&lt;/a&gt; and is available in the &lt;a href="https://operatorhub.io/"&gt;OpenShift Operator Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Service Binding Operator has two significant benefits compared to other methods of sharing secrets. The first is security: The SBO requires less exposure of credentials or secrets throughout the &lt;a href="/topics/ci-cd"&gt;CI/CD&lt;/a&gt; process. The second benefit is convenience: Rarely is development as easy as dragging a line in a graphical user interface (UI) in the &lt;a href="/openshift"&gt;Red Hat OpenShift&lt;/a&gt; console.&lt;/p&gt; &lt;p&gt;You can find more information on how the Service Binding Operator works in the article &lt;a href="/articles/2021/10/27/announcing-service-binding-operator-10-ga"&gt;Announcing Service Binding Operator 1.0 GA&lt;/a&gt;. The article &lt;a href="/articles/2022/03/28/simplify-secure-connections-postgresql-databases-nodejs"&gt;Simplify secure connections to PostgreSQL databases with Node.js&lt;/a&gt; provides information about using the SBO to share credentials among backing services and compares the technique to others, using as an example the PostgreSQL client supported by kube-service-bindings.&lt;/p&gt; &lt;p&gt;In the background, the SBO:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Passes a variable named &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; to the application environment to direct it to the credentials.&lt;/li&gt; &lt;li&gt;Projects the binding data into the application container, under the directory &lt;code&gt;/$SERVICE_BINDING_ROOT/&amp;lt;application-name&amp;gt;&lt;/code&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;What is kube-service-bindings and how does it work?&lt;/h2&gt; &lt;p&gt;kube-service-bindings finds, parses, and transforms data such as credentials into a consumable format appropriate for each client, such as a database. kube-service-bindings checks the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; environment variable to find which directory in the application instance has the binding data. The presence of the &lt;code&gt;SERVICE_BINDING_ROOT&lt;/code&gt; variable indicates that binding data is available. If the environment variable or binding data are not available, kube-service-bindings throws an error, which can easily be discovered through a try/catch block.&lt;/p&gt; &lt;p&gt;Besides parsing the data, kube-service-bindings knows exactly what it takes to provide the right configuration for each client, another advantage to using the package. Our goal in developing kube-service-bindings is to support the most common clients. We started by supporting backing services listed in the General Availability (GA) release for the Red Hat Service Binding Operator, as outlined in the section &lt;a href="/articles/2021/10/27/announcing-service-binding-operator-10-ga#extracting_the_binding_data_from_backing_services"&gt;Extracting the binding data from backing services&lt;/a&gt; of the previously mentioned article.&lt;/p&gt; &lt;p&gt;Version 1.0 of kube-service-bindings has made a good start in its support for clients. We would like to prioritize our work based on the needs of the community, so feel free to open a request in &lt;a href="https://github.com/nodeshift/kube-service-bindings/issues"&gt;the kube-service-bindings repository&lt;/a&gt; for the next client you would like to see supported.&lt;/p&gt; &lt;p&gt;Table 1 shows the currently supported clients. To connect to a client, the Node.js program issues a &lt;code&gt;getBinding&lt;/code&gt; call, passing the type (column 1) as the first argument and the client (column 2) as the second.&lt;/p&gt; &lt;div&gt; &lt;table cellspacing="0" width="NaN"&gt; &lt;caption&gt;Table 1: Clients currently supported by kube-service-bindings.&lt;/caption&gt; &lt;tbody&gt; &lt;tr&gt; &lt;/tr&gt; &lt;tr&gt; &lt;th&gt; &lt;p&gt;Type&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;Client&lt;/p&gt; &lt;/th&gt; &lt;th&gt; &lt;p&gt;Date Added&lt;/p&gt; &lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;AMQP&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/rhea"&gt;rhea&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;March 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Kafka&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/node-rdkafka"&gt;node-rdkafka&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;April 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Kafka&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/kafkajs"&gt;kafkajs&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;April 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MongoDB&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mongodb"&gt;MongoDB&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;February 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MongoDB&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mongoose"&gt;mongoose&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;June 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mysql"&gt;MySQL&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/mysql2"&gt;MySQL 2&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;MySQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/odbc"&gt;odbc&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;May 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;PostgreSQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/odbc"&gt;odbc&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;June 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;PostgreSQL&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/pg"&gt;postgres&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;December 2021&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Redis&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/redis"&gt;redis&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;January 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td width="NaN"&gt; &lt;p&gt;Redis&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;&lt;a href="https://www.npmjs.com/package/ioredis"&gt;ioredis&lt;/a&gt;&lt;/p&gt; &lt;/td&gt; &lt;td width="NaN"&gt; &lt;p&gt;January 2022&lt;/p&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;/table&gt; &lt;/div&gt; &lt;h2&gt;Simplifying access to services on Kubernetes&lt;/h2&gt; &lt;p&gt;This article has explained the roles of the Service Binding Operator and kube-service-bindings in making it easy to connect to backing services such as databases. Subsequent articles in this series will go through an example that connects a Node.js application to a database using these tools.&lt;/p&gt; The post &lt;a href="/articles/2022/08/11/connect-services-kubernetes-easily-kube-service-bindings" title="Connect to services on Kubernetes easily with kube-service-bindings"&gt;Connect to services on Kubernetes easily with kube-service-bindings&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Costas Papastathis, Michael Dawson</dc:creator><dc:date>2022-08-11T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - 11 August 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-08-11.html" /><category term="quarkus" /><category term="kubernetes" /><category term="java" /><category term="jakarta" /><category term="infinispan" /><category term="wildfly" /><category term="cloud-native" /><category term="openshift" /><category term="kogito" /><category term="drools" /><category term="keycloak" /><author><name>Don Naro</name><uri>https://www.jboss.org/people/don-naro</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-08-11.html</id><updated>2022-08-11T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, kubernetes, java, jakarta, infinispan, wildfly, cloud-native, openshift, kogito, drools, keycloak"&gt; &lt;h1&gt;This Week in JBoss - 11 August 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hi everyone! It’s great to be back and bringing you another edition of the JBoss Editorial. As always there’s a lot of exciting news and updates from JBoss communities so let’s dive in.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_release_roundup"&gt;Release roundup&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://infinispan.org/blog/2022/08/09/infinispan-14"&gt;Infinispan 14.0.0.CR1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/07/keycloak-1901-released.html"&gt;Keycloak 19.0.1&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://vertx.io/blog/eclipse-vert-x-4-3-3/"&gt;Eclipse Vert.x 4.3.3&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://resteasy.dev/2022/08/02/resteasy-6.1.0-release/"&gt;RESTEasy 6.1.0.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-11-2-final-released/"&gt;Quarkus 2.11.2.Final&lt;/a&gt; (&lt;code&gt;CVE-2022-2466&lt;/code&gt; is still ongoing)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/08/05/WildFly27-Alpha4-Released/"&gt;WildFly 27 Alpha4&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/08/kogito-1-25-0-released.html"&gt;Kogito 1.25.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_wildfly_maven_plugin_to_create_container_images"&gt;WildFly Maven plugin to create container images&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.wildfly.org/news/2022/08/04/wildfly-maven-docker/"&gt;Use the wildfly-maven-plugin to create a Docker image of your application&lt;/a&gt;, by By Jeff Mesnil&lt;/p&gt; &lt;p&gt;Jeff explains how to use the &lt;code&gt;wildlfy-maven-plugin&lt;/code&gt; and the new WildFly runtime image to build container images. The WildFly Maven plugin, currently in beta with Final planned for WildFly 27, offers a new, and very compelling, architecture to control the full runtime from the application &lt;code&gt;pom.xml&lt;/code&gt;. Developers control the full customization of WildFly using feature packs, packaging scripts, and other artifacts. This approach ensures that the runtime fits the user’s application. Creating a container image is simply a matter of putting it in a runtime image that contains OpenJDK.&lt;/p&gt; &lt;p&gt;The WildFly team are starting an open conversation to bring additional synergies between the Docker and S2I images for WildFly that could benefit the whole community. The team are aiming to bring new capabilities, additional architectures (in particular &lt;code&gt;linux/arm64&lt;/code&gt;), and newer versions of the JDK to all WildFly images. Be sure to check out Jeff’s post and find out how you can get involved!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_jakarta_bean_validation"&gt;Jakarta Bean Validation&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/java-ee/validation/test/"&gt;Getting started with Jakarta Bean Validation&lt;/a&gt;, by Francesco Marchioni&lt;/p&gt; &lt;p&gt;Francesco takes a look at the Jakarta Bean Validation specification which allows you to express constraints on your model and create custom ones in an extensible way. His detailed post shows you how to write a constraint once and use it in any application layer. Given that Bean validation is layer agnostic, meaning that you can use the same constraint from the presentation to the business model layer.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_kogito_rules_drools_with_java_inheritance"&gt;Kogito Rules (Drools) with Java Inheritance&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/08/kogito-rules-drools-with-java-inheritance.html"&gt;Kogito Rules (Drools) with Java Inheritance&lt;/a&gt;, by Jeff Taylor&lt;/p&gt; &lt;p&gt;In this article, Jeff explains how Kogito rules services can reason over application domain model facts that are represented using plain old Java objects, or POJOs, that use standard Java inheritance. DRL rules files can use POJOs as well as client applications that call the Kogito rules services.&lt;/p&gt; &lt;p&gt;Jeff explores two approaches for sharing Java subclasses between a Kogito rules service and a client application. The first approach isolates objects from each subclass into a JSON array while the second approach uses Jackson inheritance annotations to embed objects from each subclass for REST API calls that serialize and deserialize POJOs to and from JSOn.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_welcome_václav_muzikář"&gt;Welcome Václav Muzikář!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/08/vaclav"&gt;New Keycloak maintainer: Václav Muzikář&lt;/a&gt;, by Bruno Oliveira&lt;/p&gt; &lt;p&gt;The Keycloak team has a new community maintainer! Hearty welcome to Václav Muzikář.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_youtube_videos"&gt;YouTube videos&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;From unmissable demos to brilliant chat about the latest Java trends, the JBoss community has some great video content for you:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/Urj1X60H6YY"&gt;Quarkus Insights #98: Using Minecraft as an Observability client&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/nH-27gOp0h4"&gt;Quarkus Insights #97: Qute with Quarkus&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/kdasoBPOWUQ"&gt;Quarkus Insights #96: Quarkus Q&amp;#38;A&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://youtu.be/9DMAkrM_gOA"&gt;MLOps with Flyte with Samhita Alla&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_see_you_next_time"&gt;See you next time&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;&lt;em&gt;Hope you enjoyed this edition. Please join us again in two weeks for our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/don-naro.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Don Naro&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Don Naro</dc:creator></entry><entry><title type="html">Kogito Rules (Drools) with Java Inheritance</title><link rel="alternate" href="https://blog.kie.org/2022/08/kogito-rules-drools-with-java-inheritance.html" /><author><name>Jeff Taylor</name></author><id>https://blog.kie.org/2022/08/kogito-rules-drools-with-java-inheritance.html</id><updated>2022-08-10T11:36:00Z</updated><content type="html">COMPARISON OF A JSON ARRAY BASED APPROACH VS JACKSON INHERITANCE ANNOTATIONS -------------------------------------------------------------------------------- INTRODUCTION: “Kogito is a next generation business automation toolkit that originates from well known Open Source projects Drools (for business rules) and jBPM (for business processes). Kogito aims at providing another approach to business automation where the main message is to expose your business knowledge (processes, rules and decisions) in a domain specific way.” (4) Kogito rules services can reason over application domain model facts that are represented using “plain old Java objects” (POJO’s). The POJO’s can be used in DRL rules files and additionally the POJO’s may be used by the client applications that call the Kogito rules services. The communication layer between the rules service and the client application often uses RestAPI calls where the POJO’s are serialized and deserialize, to and from JSON. The POJO’s may use standard Java inheritance. This paper explores two approaches for sharing Java subclasses between the rules service and the client application. The first approach uses JSON arrays to isolate the objects from each subclass into its own array. The second approach uses Jackson inheritance annotations so that the subclass of every object will be embedded during the RestAPI request and response. Examples of the two approaches are available here: * * BACKGROUND: “For any rule base application, a fact model is needed to drive the rules. The fact model typically overlaps with the applications domain model, but in general it will be decoupled from it (as it makes the rules easier to manage over time). There are no technical limitations on using your domain model as your fact model, however this introduces tighter coupling between your business domain (domain model) and your knowledge domain (fact model). Consequentially if your domain model were to change you would need to, at the very least, revisit your rule definitions.” (8) “Red Hat Decision Manager supports several assets that you can use to define business decisions for your decision service. Each decision-authoring asset has different advantages, and you might prefer to use one or a combination of multiple assets depending on your goals and needs. DRL (Drools Rule Language) rules are business rules that you define directly in .drl text files.” (9) EXAMPLE SHARED FACT INHERITANCE MODEL: Class diagram for the common fact model Goal: Create DRL rules to identify the overloaded cars and trucks. First Approach: Each payload includes a JSON array of each subclass: First Approach superclass: package com.example.vehicle.datamodel; @lombok.Getter @lombok.Setter public class Vehicle { private String color; private Integer vehicleId; private Boolean overloaded = false; } First Approach example JSON payload Five vehicles: One generic , two cars and two trucks. Notice that although every instance shares the same superclass, instances of every subclass are isolated into their own JSON array. { "vehicleInstances": [ { "color": "red", "vehicleId": 1 } ], "carInstances": [ { "color": "bright green", "vehicleId": 2, "currentPassengers": 5, "maxPassengers": 4 }, { "color": "lime green", "vehicleId": 3, "currentPassengers": 2, "maxPassengers": 5 } ], "truckInstances": [ { "color": "medium blue", "vehicleId": 4, "currentCargoWeight": 5000, "maxCargoWeight": 4000 }, { "color": "navy blue", "vehicleId": 5, "currentCargoWeight": 2000, "maxCargoWeight": 5000 } ] } First Approach: Rule Unit Data for JSON array of each subclass Set up the rule unit data to receive the arrays of subclasses: public class VehicleUnitData implements RuleUnitData { public DataStore&lt;Vehicle&gt; vehicleInstances = DataSource.createStore(); public DataStore&lt;Car&gt; carInstances = DataSource.createStore(); public DataStore&lt;Truck&gt; truckInstances = DataSource.createStore(); } First Approach: Rules to work with list of subclasses rule "Car Rule using list of subclasses" when $c : /carInstances[ currentPassengers &gt; maxPassengers ] then modify($c){setOverloaded(true)}; end rule "Truck Rule using list of subclasses" when $t : /truckInstances[currentCargoWeight &gt; maxCargoWeight] then modify($t){setOverloaded(true)}; end query "GetOverloadedCars" $c: /carInstances[overloaded] end query "GetOverloadedTrucks" $t: /truckInstances[overloaded] end query "GetOverloadedVehicles" $t: /vehicleInstances[overloaded] end First Approach usage: ## Call the Car RestAPI endpoint $ cat VehicleAppList/src/main/resources/payload.json | curl -s -d@- -H "Content-type: application/json" http:/localhost:8080/get-overloaded-cars | jq [ { "color": "bright green", "vehicleId": 2, "overloaded": true, "currentPassengers": 5, "maxPassengers": 4 } ] ## Call the Truck RestAPI endpoint $ cat VehicleAppList/src/main/resources/payload.json | curl -s -d@- -H "Content-type: application/json" http:/localhost:8080/get-overloaded-trucks | jq [ { "color": "medium blue", "vehicleId": 4, "overloaded": true, "currentCargoWeight": 5000, "maxCargoWeight": 4000 } ] -------------------------------------------------------------------------------- Second Approach: Using Jackson Inheritance Annotations so that each payload includes an attribute to self identify it’s own subclass: Second Approach superclass: package com.example.vehicle.datamodel; import com.fasterxml.jackson.annotation.JsonSubTypes; import com.fasterxml.jackson.annotation.JsonSubTypes.Type; import com.fasterxml.jackson.annotation.JsonTypeInfo; @lombok.Getter @lombok.Setter @JsonTypeInfo( use = JsonTypeInfo.Id.NAME, include = JsonTypeInfo.As.PROPERTY, property = "vehicleType", visible = true) @JsonSubTypes({ @Type(value = Car.class, name = "Car"), @Type(value = Truck.class, name = "Truck") }) public class Vehicle { private String color; private Integer vehicleId; private Boolean overloaded = false; private String vehicleType; } Second Approach example JSON payload Five vehicles: One generic , two cars and two trucks. Notice that every instance identifies it’s own subclass. { "vehicleInstances": [ { "vehicleType": "Vehicle", "color": "red", "vehicleId": 1 }, { "vehicleType": "Car", "color": "bright green", "vehicleId": 2, "currentPassengers": 5, "maxPassengers": 4 }, { "vehicleType": "Car", "color": "lime green", "vehicleId": 3, "currentPassengers": 2, "maxPassengers": 5 }, { "vehicleType": "Truck", "color": "medium blue", "vehicleId": 4, "currentCargoWeight": 5000, "maxCargoWeight": 4000 }, { "vehicleType": "Truck", "color": "navy blue", "vehicleId": 5, "currentCargoWeight": 2000, "maxCargoWeight": 5000 } ] } Second Approach: Rule Unit Data for JSON array of the superclass package com.example.vehicle.rules; import com.example.vehicle.datamodel.Vehicle; import org.kie.kogito.rules.DataSource; import org.kie.kogito.rules.DataStore; import org.kie.kogito.rules.RuleUnitData; @lombok.Getter @lombok.Setter public class VehicleUnitData implements RuleUnitData { public DataStore&lt;Vehicle&gt; vehicleInstances = DataSource.createStore(); } Second Approach: Rules to work with the subclasses package com.example.vehicle.rules; unit VehicleUnitData; import com.example.vehicle.datamodel.Car; import com.example.vehicle.datamodel.Truck; rule "Car Rule" when $v : /vehicleInstances#Car[ currentPassengers &gt; maxPassengers ] then modify($v){setOverloaded(true)}; end rule "Truck Rule" when $v : /vehicleInstances#Truck[ currentCargoWeight &gt; maxCargoWeight ] then modify($v){setOverloaded(true)}; end query "GetOverloadedVehicles" $v: /vehicleInstances[overloaded] end Second Approach usage: $ cat VehicleAppPoly/src/main/resources/payload.json | curl -s -d@- -H "Content-type: application/json" http:/localhost:8080/get-overloaded-vehicles | jq [ { "vehicleType": "Car", "color": "bright green", "vehicleId": 2, "overloaded": true, "currentPassengers": 5, "maxPassengers": 4 }, { "vehicleType": "Truck", "color": "medium blue", "vehicleId": 4, "overloaded": true, "currentCargoWeight": 5000, "maxCargoWeight": 4000 } ] Conclusion: Jackson inheritance annotations can be used to track the type of Java subclasses of that will be used as facts for the Kogito rules engine. APPENDIX: NOTES ON THE CREATION OF THE MAVEN MODULES: CONSTRUCT THE PARENT: a. Create the parent maven module $ quarkus create app --no-code com.example.vehicle:VehicleApp:2.0.0-SNAPSHOT b. Change the packaging to pom type and add lombok. Edit VehicleApp/pom.xml ... &lt;packaging&gt;pom&lt;/packaging&gt; &lt;properties&gt; ... &lt;lombok.version&gt;1.18.24&lt;/lombok.version&gt; &lt;/properties&gt; ... &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;${lombok.version}&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2. CONSTRUCT THE DATA MODEL a. Create the datamodel maven project $ cd VehicleApp/ $ quarkus ext add quarkus-resteasy quarkus-resteasy-jackson $ mvn archetype:generate \ -DarchetypeArtifactId=maven-archetype-quickstart \ -DarchetypeVersion=1.4 \ -DgroupId=com.example.vehicle \ -DartifactId=datamodel \ -Dversion=2.0.0-SNAPSHOT b. Directory: $ mkdir -p datamodel/src/main/java/com/example/vehicle/datamodel c. Vehicle superclass: package com.example.vehicle.datamodel; @lombok.Getter @lombok.Setter public class Vehicle { private String color; private Integer vehicleId; private Boolean overloaded = false; } d. Car subclass package com.example.vehicle.datamodel; @lombok.Getter @lombok.Setter public class Car extends Vehicle { private Integer currentPassengers; private Integer maxPassengers; } e. Truck subclass package com.example.vehicle.datamodel; @lombok.Getter @lombok.Setter public class Truck extends Vehicle { private Integer currentCargoWeight; private Integer maxCargoWeight; } 3. CONSTRUCT THE VEHICLE-DECISION-SERVICE MAVEN PROJECT a. Create the Kogito Rules project mvn io.quarkus.platform:quarkus-maven-plugin:2.11.1.Final:create \ -DprojectGroupId=com.example.vehicle \ -DprojectArtifactId=vehicle-decision-service \ -Dversion=2.0.0-SNAPSHOT \ -Dextensions="kogito-quarkus-rules,quarkus-resteasy,quarkus-resteasy-jackson,quarkus-smallrye-openapi" b. Add directories for the DRL files and the RuleUnit mkdir vehicle-decision-service/src/main/resources/vehicle mkdir vehicle-decision-service/src/main/java/vehicle/ REFERENCES: 1. “Design Patterns in Production Systems” () 2. “Using DRL rules in Kogito services” () 3. “Drools Documentation” () 4. “Using Kogito to add rule engine capabilities to an application” () 5. “Writing Json Rest Services” () 6. “Inheritance with Jackson” () 7. “Automating rule-based services with Java and Kogito” () 8. “The Fact Model” () 9. “Decision-authoring assets in Red Hat Decision Manager” () The post appeared first on .</content><dc:creator>Jeff Taylor</dc:creator></entry><entry><title>How to configure Podman 4.0 for IPv6</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/10/how-conifgure-podman-40-ipv6" /><author><name>Ranjith Rajaram</name></author><id>190aad79-396f-4282-abbd-1cdc361872fa</id><updated>2022-08-10T07:00:00Z</updated><published>2022-08-10T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt; is a major &lt;a href="/topics/containers"&gt;container&lt;/a&gt; platform, &lt;a href="/articles/2022/05/02/podman-basics-resources-beginners-and-experts"&gt;used by many developers&lt;/a&gt; in place of Docker. &lt;a href="https://podman.io/releases/2022/02/22/podman-release-v4.0.0.html"&gt;Podman v4.0&lt;/a&gt; has extensive new support for the IPv6 address format. IPv6 networks with Network Address Translation (NAT) and port forwarding are now fully tested and supported in this latest version of the platform. You can also assign static IPv6 addresses to containers in these networks.&lt;/p&gt; &lt;p&gt;Podman v4.0 is supported in versions 8.6 and 9 of &lt;a href="/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;. This article shows you how to enable the new IPv6 support.&lt;/p&gt; &lt;h2&gt;Changes to Podman&lt;/h2&gt; &lt;p&gt;The reason you need to take special steps to enable IPv6 is that Podman went through a major architectural change in version 4.0. The new network stack, which has been rewritten from scratch in Rust, is composed of two tools:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;strong&gt;Netavark:&lt;/strong&gt; A network setup tool that configures network bridges, firewall rules, and system settings to give containers access to external networks&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Aardvark:&lt;/strong&gt; An authoritative DNS server for A and AAAA container records, enabling containers to resolve connections to other containers by their names or aliases.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Both these tools work together to enable container networking with IPv6, but they are not enabled by default.&lt;/p&gt; &lt;p&gt;For more information about the new network stack, please refer to the article &lt;a href="https://www.redhat.com/sysadmin/podman-new-network-stack"&gt;Podman 4.0's new network stack: What you need to know&lt;/a&gt;.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Podman v4.0 performs several schema migrations in the Podman database during the first run. These schema migrations prevent Podman v3.x and earlier from reading some network configuration information from the database. Therefore, downgrading from Podman v4.0 to an earlier version will cause containers to lose their static IP, MAC address, and port bindings.&lt;/p&gt; &lt;h2&gt;Package installation and configuration&lt;/h2&gt; &lt;p&gt;If you're using Red Hat Enterprise Linux 8.6, you'll need to take a few additional steps to install the Netavark backend; in Red Hat Enterprise Linux 9, Netavark is installed along with the Podman package.&lt;/p&gt; &lt;h3&gt;Install Podman 4.0 on Red Hat Enterprise Linux 8.6&lt;/h3&gt; &lt;p&gt;Before upgrading the Podman package from v3.x to 4.x on RHEL 8.6, you should remove all non-default defined networks. You can recreate them after the package upgrade.&lt;/p&gt; &lt;p&gt;On RHEL 8.6, by default Podman will use the Container Network Interface (CNI) backend. In order to use Netavark instead, you must explicitly mention it during installation, as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# yum install podman netavark -y&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Aardvark package will be installed as a dependency of the Netavark package, so you don't have to mention it in the command.&lt;/p&gt; &lt;p&gt;Then copy the &lt;code&gt;containers.conf&lt;/code&gt; file to &lt;code&gt;/etc/containers/&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# cp /usr/share/containers/containers.conf /etc/containers/containers.conf&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Edit &lt;code&gt;/etc/containers/containers.conf&lt;/code&gt; and find the following line under the &lt;code&gt;[network]&lt;/code&gt; section:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;network_backend = "cni"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Change that line to:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;network_backend = "netavark"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can issue the following commands to verify that Podman will use the Netavark backend:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman info |grep -i networkbackend networkBackend: netavark [root@atomic-test ~]# cat /var/lib/containers/storage/defaultNetworkBackend netavark&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Podman 4.0 on Red Hat Enterprise Linux 9&lt;/h3&gt; &lt;p&gt;If you're using RHEL 9, the process is much simpler. The Netavark backend is installed along with Podman with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; [root@atomic-test ~]# yum install podman -y &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The Netavark and Aardvark packages are installed as dependencies of Podman, so you don't have to mention them in the command. And Podman uses Netavark as the network backend by default in RHEL 9, so no further configuration is required.&lt;/p&gt; &lt;h2&gt;Create an interface that supports the dual IPv4/IPv6 stack&lt;/h2&gt; &lt;p&gt;The default bridge, &lt;code&gt;podman0&lt;/code&gt;, supports only the IPv4 stack, and DNS is disabled. A look at the default stack shows that IPv6 and DNS are disabled:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;# podman network inspect podman [ { "name": "podman", "id": "2f259bab93aaaaa2542ba43ef33eb990d0999ee1b9924b557b7be53c0b7a1bb9", "driver": "bridge", "network_interface": "podman0", "created": "2022-06-24T18:49:34.800035098+05:30", "subnets": [ { "subnet": "10.88.0.0/16", "gateway": "10.88.0.1" } ], "ipv6_enabled": false, "internal": false, "dns_enabled": false, "ipam_options": { "driver": "host-local" } } ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You must therefore create a new bridge that supports IPv6 addresses and DNS. Here, we'll name this bridge &lt;code&gt;podman1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman network create --ipv6 podman1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Examine the new bridge by issuing this command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman network inspect podman1&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This snippet from the output shows the use of IPv6:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; { "subnet": "fd96:7c2e:b8d2:bf65::/64", "gateway": "fd96:7c2e:b8d2:bf65::1" } ], "ipv6_enabled": true,&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now create a network interface by specifying the network range for both IPv4 and IPv6:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman network create --ipv6 --gateway fd00::1:8:1 --subnet fd00::1:8:0/112 --gateway 10.90.0.1 --subnet 10.90.0.0/16 podman1&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Attach a pod to the Podman network interface&lt;/h2&gt; &lt;p&gt;Podman is now configured to handle pods using IPv6. Run a sample pod as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman run --network podman1 -d -p 8080:80 docker.io/fedora/apache&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Expose the pod with a static IPv6 address, using the new &lt;code&gt;--ip6&lt;/code&gt; option:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# podman run --network podman1 -d --ip6 fd00::1:8:9 -p 8080:80 docker.io/fedora/apache&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Port mapping&lt;/h2&gt; &lt;p&gt;To make the container service reachable from an outside network, Netavark creates appropriate masquerade rules in iptables. For the static pod created in the previous section, let's take a quick look at the associated DNAT rules using the following &lt;code&gt;iptables&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt; iptables -t nat -L -n -v &lt;/code&gt; &lt;/pre&gt; &lt;p&gt;The output should look like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;Chain PREROUTING (policy ACCEPT) target prot opt source destination NETAVARK-HOSTPORT-DNAT all ::/0 ::/0 ADDRTYPE match dst-type LOCAL Chain NETAVARK-HOSTPORT-DNAT (2 references) target prot opt source destination NETAVARK-DN-F11DC6A6D09CF tcp ::/0 ::/0 tcp dpt:8080 /* dnat name: podman1 id: 8de28bebed939bb6449fe3d97cce5ae2e4a785462bb9ffa8d1417143f809bff0 */ Chain NETAVARK-DN-F11DC6A6D09CF (1 references) target prot opt source destination NETAVARK-HOSTPORT-SETMARK tcp fd00::1:8:0/112 ::/0 tcp dpt:8080 NETAVARK-HOSTPORT-SETMARK tcp ::1 ::/0 tcp dpt:8080 DNAT tcp ::/0 ::/0 tcp dpt:8080 to:[fd00::1:8:9]:80&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When a packet destined for container port 8080 enters the host network, the packet is first processed by the iptables &lt;code&gt;PREROUTING&lt;/code&gt; chain. Packets are then further processed by the &lt;code&gt;NETAVARK-HOSTPORT-DNAT&lt;/code&gt; chain (the only custom chain defined in the &lt;code&gt;PREROUTING&lt;/code&gt; chain). Depending on the port used, packets are further processed by the port-specific DNAT chain &lt;code&gt;NETAVARK-DN-F11DC6A6D09CF&lt;/code&gt; defined in &lt;code&gt;NETAVARK-DN-F11DC6A6D09CF&lt;/code&gt;. Packets are then sent to the appropriate container.&lt;/p&gt; &lt;h2&gt;Aardvark DNS server&lt;/h2&gt; &lt;p&gt;Aardvark keeps track of the containers and their assigned IP addresses based on the network interface to which the pod has been attached. Containers attached to the same network are resolvable using their names.&lt;/p&gt; &lt;p&gt;The following example uses the &lt;code&gt;podman1&lt;/code&gt; bridge we previously created:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[root@atomic-test ~]# ls /run/containers/networks/aardvark-dns/ aardvark.pid podman &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Like DNS zone records, Aardvark keeps track of what is similar to DNS A/AAA records. The mapping of container names to IP addresses is stored in this file. If a container is not reachable using its container name, you can refer to this file to see if a record has been created for that container.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;/run/containers/networks/aardvark-dns/podman1&lt;/code&gt; file shows DNS records for &lt;code&gt;podman1&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;fd00::1:8:1 0f6d61bb7f6e8dcb4b8586ebc90d699b0e988406240419e8e48e1082ffd00451 fd00::1:8:2 goofy_cannon,0f6d61bb7f6e 0f6d61bb7f6e8dcb4b8586ebc90d699b0e988406240419e8e48e1082ffd00451 10.90.0.2 goofy_cannon,0f6d61bb7f6e 75647734d979ab6ded5636741ccd794fd1845fe0d5ef7161071fe9bd93d7f1fa fd00::1:8:9 test1,75647734d979 75647734d979ab6ded5636741ccd794fd1845fe0d5ef7161071fe9bd93d7f1fa 10.90.0.3 test1,75647734d979 e394edc2e5bda1e790785d07eb32f8ec72e010f0dac5978e2152df472291d715 fd00::1:8:10 test2,e394edc2e5bd e394edc2e5bda1e790785d07eb32f8ec72e010f0dac5978e2152df472291d715 10.90.0.4 test2,e394edc2e5bd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Whenever a pod is deleted, Aardvark removes the entry from its database.&lt;/p&gt; &lt;h2&gt;Rootless containers&lt;/h2&gt; &lt;p&gt;Rootless containers continue to use the &lt;a href="https://github.com/rootless-containers/slirp4netns"&gt;slirp4netns&lt;/a&gt; service for communication. To expose the container service externally, slirp4netns listens on the host network according to the port mapping configuration. We want rootless containers to be reachable using IPV6 addresses as well.&lt;/p&gt; &lt;p&gt;By default, when a Podman container is started, it does not get an IP address. Containers within the host should be reachable using the mapped port in the format &lt;code&gt;host_ip&lt;/code&gt;:&lt;code&gt;port&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;For rootless containers, you can define custom Podman network interfaces as the rootless user and then attach the pod to them. Containers should be reachable within the host using the container IP address.&lt;/p&gt; &lt;p&gt;To enable IPv6 addresses with a custom network, enter:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[awx@atomic-test ~]$ podman network create --ipv6 redhat &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Inspect the newly created &lt;code&gt;redhat&lt;/code&gt; interface as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[awx@atomic-test ~]$ podman network inspect redhat [ { "name": "redhat", "id": "262e49487783d58774dfa5f581e07849583558c059416d014774317ffc4190a7", "driver": "bridge", "network_interface": "podman1", "created": "2022-06-29T05:30:24.968361698-04:00", "subnets": [ { "subnet": "10.89.0.0/24", "gateway": "10.89.0.1" }, { "subnet": "fd25:1552:57a6:f8ee::/64", "gateway": "fd25:1552:57a6:f8ee::1" } ], "ipv6_enabled": true, "internal": false, "dns_enabled": true, "ipam_options": { "driver": "host-local" } } ]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Attach a rootless container to the custom network with a static IPv6 address as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="bash"&gt;[awx@atomic-test ~]$ podman run --network redhat -d --ip6 fd25:1552:57a6:f8ee::10 -p 8081:80 docker.io/fedora/apache&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Rootless containers within the host should be reachable by their own IP address or by a static IP address. External communication for such containers will continue to be handled by slirp4netns.&lt;/p&gt; &lt;p&gt;Aardvark DNS is enabled only for custom network interfaces, created using &lt;code&gt;podman network create&lt;/code&gt;. Aardvark takes care of resolution for rootless containers using their respective container names attached to the same custom network interface.&lt;/p&gt; &lt;h2&gt;More in Podman 4.0&lt;/h2&gt; &lt;p&gt;Along with IPv6 support, the new network stack included with Podman v4.0 features improved support for containers in multiple networks and improved performance. Podman also continues to support the CNI stack. There are several other features which are not discussed here, such as the &lt;code&gt;podman network connect&lt;/code&gt; command, which allows you to connect a running container to another network. So explore the shiny new network stack, and leave a comment on this article if you have thoughts about the improvements in IPv6 support.&lt;/p&gt; The post &lt;a href="/articles/2022/08/10/how-conifgure-podman-40-ipv6" title="How to configure Podman 4.0 for IPv6"&gt;How to configure Podman 4.0 for IPv6&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Ranjith Rajaram</dc:creator><dc:date>2022-08-10T07:00:00Z</dc:date></entry><entry><title type="html">Infinispan 14.0.0.CR1</title><link rel="alternate" href="https://infinispan.org/blog/2022/08/09/infinispan-14" /><author><name>Tristan Tarrant</name></author><id>https://infinispan.org/blog/2022/08/09/infinispan-14</id><updated>2022-08-09T12:00:00Z</updated><content type="html">Dear Infinispan community, Infinispan 14 candidate release 1 is here! Here is your chance to verify your application against our latest and greatest and tell us if things are working as expected or if there are any showstoppers we should address before tagging the final release. AARCH64 IMAGES We are now building images for AArch64 (aka ARM64) by default, which allow you to run Infinispan on Apple Silicon, Amazon Graviton and other ARM CPU platforms without resorting to emulation. PROTOBUF ONEOF SUPPORT Protostream has been updated to support ProtoBuf 3’s oneof keyword. CONSOLE When creating a new cache, it’s now possible to choose the key/value types from the list of available protobuf schemas. CLI The CLI can now connect to a server secured with client certificate authentication. The config command now supports the keystore and keystore-password to persist the client certificate configuration. Additionally, the new config reset command offers a quick way to reset all configuration properties to their default values. RELEASE NOTES You can look at the to see what has changed. Get them from our .</content><dc:creator>Tristan Tarrant</dc:creator></entry><entry><title>8 elements of securing Node.js applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications" /><author><name>Lucas Holmquist</name></author><id>2526777f-7b53-4a26-88f8-8bf100b7ef8e</id><updated>2022-08-09T07:00:00Z</updated><published>2022-08-09T07:00:00Z</published><summary type="html">&lt;p&gt;Making your &lt;a href="/topics/nodejs"&gt;Node.js&lt;/a&gt; applications secure is an essential part of the development of Node.js modules and applications. Security practices apply to both the code itself and your software development process. This installment of the ongoing &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js Reference Architecture&lt;/a&gt; series focuses on some of the key security elements that &lt;a href="/topics/javascript"&gt;JavaScript&lt;/a&gt; developers should address.&lt;/p&gt; &lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;Overview of the Node.js reference architecture&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;Logging in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;Code consistency in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;GraphQL in Node.js&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;Choosing web frameworks&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;Code Coverage&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;Typescript&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;This article covers eight key elements of building security into your software development process to make your Node.js applications and modules robust:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Choosing dependencies&lt;/li&gt; &lt;li&gt;Managing access and content of public and private data stores such as npm and GitHub&lt;/li&gt; &lt;li&gt;Writing defensive code&lt;/li&gt; &lt;li&gt;Limiting required execution privileges&lt;/li&gt; &lt;li&gt;Support for logging and monitoring&lt;/li&gt; &lt;li&gt;Externalizing secrets&lt;/li&gt; &lt;li&gt;Maintaining a secure and up-to-date foundation for deployed applications&lt;/li&gt; &lt;li&gt;Maintaining individual modules&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Although this is not necessarily an exhaustive list, these are commonly the focus of the Red Hat and IBM teams.&lt;/p&gt; &lt;h2&gt;1. Choosing third-party dependencies&lt;/h2&gt; &lt;p&gt;Most Node.js applications and modules have third-party dependencies, many of which contain security vulnerabilities. Although open source teams usually fix the vulnerabilities soon after discovery, there are still gaps in time before an application developer learns about the vulnerability and puts the fixed library into production. Attackers might exploit the compromised program during those times. So it is important to choose dependencies carefully and regularly evaluate if they remain the right choices for you.&lt;/p&gt; &lt;p&gt;A couple of helpful tips in this area are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Determine that a dependency is necessary before integrating it into your application. Is using the modules instead of your code saving development and maintenance time?&lt;/li&gt; &lt;li&gt;Avoid code one-liners.&lt;/li&gt; &lt;li&gt;If you have a choice of dependencies, use one that has only a few or no dependencies of its own.&lt;/li&gt; &lt;li&gt;Choose dependencies that already have a high level of usage based on statistics, such as GitHub stars and npm. These tend to be maintained well.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Find more in-depth guidance on managing dependencies in the reference architecture's &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/dependencies.md" target="_blank"&gt;choosing and vetting dependencies&lt;/a&gt; section.&lt;/p&gt; &lt;h2&gt;2. Managing access and content of public and private data stores&lt;/h2&gt; &lt;p&gt;Modern development flows often use public and private data stores, including npm and GitHub. We recommend the following management practices:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Enable two-factor authentication (2FA) to ensure the integrity of the committed code and published assets. GitHub, for instance, now requires a developer who logs in to verify their identity through a code sent to their device.&lt;/li&gt; &lt;li&gt;Use files such as &lt;code&gt;.npmignore&lt;/code&gt; and &lt;code&gt;.gitignore&lt;/code&gt; to avoid accidentally publishing secrets. These are hidden files consulted by programs (npm and Git, respectively). If you list a file with your secrets in one of these hidden files, npm and Git will never check it into the source repository. Of course, you must have a separate process to manage the secrets. There are many services available to help you.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;A &lt;code&gt;.npmrc&lt;/code&gt; file is often needed for npm installations, particularly if you have private modules. Avoid leaking information in the &lt;code&gt;.npmrc&lt;/code&gt; file when building containers by using one of these options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Use two-stage builds, where you build one image with all the tools for the application and a second to create a stripped-down image. In addition to saving memory and disk space, the two-stage build allows you to omit the &lt;code&gt;.npmrc&lt;/code&gt; file from the final image that goes into production.&lt;/li&gt; &lt;li&gt;Avoid adding the secrets to any image in the build process. Instead, you can securely mount secrets into containers during the build process, as explained in the article &lt;a href="https://projectatomic.io/blog/2018/06/sneak-secrets-into-containers"&gt;How to sneak secrets into your containers&lt;/a&gt;. In particular, &lt;a href="https://buildah.io"&gt;Buildah&lt;/a&gt; has built-in functions to make it easier to mount files with secrets.&lt;/li&gt; &lt;li&gt;The least preferred method:  Delete the &lt;code&gt;.npmrc&lt;/code&gt; file from the final image and compress images to flatten layers.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;3. Writing defensive code&lt;/h2&gt; &lt;p&gt;Secure coding often calls for special training and cannot be summarized in simple precepts. Nevertheless, you can eliminate many common vulnerabilities by following the recommendations in this section. There is a more extensive list in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/secure-development-process.md"&gt;Secure Development Process&lt;/a&gt; section of the reference architecture.&lt;/p&gt; &lt;h3&gt;Avoid global state&lt;/h3&gt; &lt;p&gt;Using global variables makes it easy to leak information between requests accidentally. With global variables, data from one web visitor might be in memory when a second visitor sends a request. Potential impacts include corrupting the request or revealing private information to another visitor.&lt;/p&gt; &lt;p&gt;Each request should encapsulate its data. If you need global data, such as statistics about the traffic you are handling, store it in an external database. This solution is preferable to global variables because the data in the database is persistent.&lt;/p&gt; &lt;h3&gt;Set the NODE_ENV environment variable to production&lt;/h3&gt; &lt;p&gt;Some packages consult the NODE_ENV environment variable to decide whether they need to lock things down or share less information. Therefore, setting the variable to &lt;code&gt;production&lt;/code&gt; is the safest setting and should be used all the time. The application developer, not the package, should determine what information to display.&lt;/p&gt; &lt;h3&gt;Validate user input&lt;/h3&gt; &lt;p&gt;Unvalidated input can result in attacks such as command injection, SQL injection, and denial of service, disrupting your service and corrupting data. Always validate user input before implementing it within your application code. Make sure you validate input on the server even if you validate on the client side (browser or mobile application) because an attacker could send requests directly to the APIs without using the client.&lt;/p&gt; &lt;h3&gt;Include good exception handling&lt;/h3&gt; &lt;p&gt;Basic practices for handling exceptions include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Check at a high level for missed exceptions and handle them gracefully. Make sure to have a default handler for &lt;a href="https://expressjs.com"&gt;Express&lt;/a&gt; and other web frameworks to avoid displaying errors with the stack trace to the visitor.&lt;/li&gt; &lt;li&gt;Listen to errors when using &lt;a href="https://nodejs.dev/learn/the-nodejs-event-emitter"&gt;EventEmitters&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Check for errors passed into asynchronous calls.&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Avoid complex regular expressions&lt;/h3&gt; &lt;p&gt;Regular expressions help with text parsing tasks, such as ensuring that a visitor submitted their email address or phone number in an acceptable format or checking input for suspicious characters that could signal an attack. Unfortunately, if a regular expression is complex, it can take a long time to run. In fact, some regexes run essentially forever on certain kinds of text.&lt;/p&gt; &lt;p&gt;Even worse, although your regular expression might operate reasonably under most input, a malicious attacker could provide content that triggers an endless run. The article &lt;a href="https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS"&gt;Regular expression Denial of Service - ReDoS&lt;/a&gt; explains this type of vulnerability.&lt;/p&gt; &lt;p&gt;The takeaway is to be careful about the complexity of any regular expression you use.  When checking text input, avoid regular expressions or use only simple ones that check for issues such as invalid characters.&lt;/p&gt; &lt;h3&gt;Limit the attack surface&lt;/h3&gt; &lt;p&gt;Some helpful ways to limit the available attack surface are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Expose only the APIs needed to support the intended operations. For example, when using Express, remove any unnecessary routes.&lt;/li&gt; &lt;li&gt;Group all external endpoints under a prefix (i.e., &lt;code&gt;/api&lt;/code&gt;). This makes it easier to expose only APIs intended to be external in the ingress configuration.&lt;/li&gt; &lt;li&gt;Don't rewrite paths to the root (&lt;code&gt;/&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Use authentication to limit access. When possible, integrate an organizational identity and access control provider instead of implementing your own.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;4. Limiting required execution privileges&lt;/h2&gt; &lt;p&gt;Design your applications to run with the minimum privileges required. Ensure that your applications can run as a non-root user, especially when deployed within containers. The user and group under which the application runs should have access only to a minimal set of files and resources. For more container recommendations, check out part five of this series:  &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;Building good containers&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;5. Support for logging and monitoring&lt;/h2&gt; &lt;p&gt;Logging sensitive or suspicious actions will make it easier for monitoring tools to collect and analyze the data. See the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;logging&lt;/a&gt; section of the reference architecture for recommended monitoring packages.&lt;/p&gt; &lt;h2&gt;6. Externalizing secrets&lt;/h2&gt; &lt;p&gt;Secrets (i.e., passwords) should be defined externally and made available to the application at runtime through secure means. Make sure you don't commit secrets in code repositories or build them into container images.&lt;/p&gt; &lt;p&gt;The article &lt;a href="https://cloud.redhat.com/blog/gitops-secret-management"&gt;GitOps secret management&lt;/a&gt; provides a good overview of the techniques and components used to manage externalized secrets. The article also refers to additional articles on the topic.&lt;/p&gt; &lt;p&gt;More specific to Node.js deployments, consider using the &lt;a href="https://www.npmjs.com/package/dotenv"&gt;dotenv&lt;/a&gt; package, which is popular among our team. We also contribute to &lt;a href="https://www.npmjs.com/package/kube-service-bindings"&gt;kube-service-bindings&lt;/a&gt; to support the &lt;a href="https://github.com/servicebinding/spec"&gt;Service Binding Specification for Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;One of the leading tools for managing externalized secrets is &lt;a href="https://www.npmjs.com/package/node-vault"&gt;node-vault&lt;/a&gt;. Teams involved in deployments with the IBM cloud find the &lt;a href="https://www.npmjs.com/package/@ibm-cloud/secrets-manager"&gt;IBM Cloud Secrets Manager Node.js SDK&lt;/a&gt; helpful.&lt;/p&gt; &lt;h2&gt;7. Maintaining a secure and up-to-date foundation for deployed applications&lt;/h2&gt; &lt;p&gt;A Node.js application is on top of several components. You must keep this foundation secure and up to date throughout your application's lifetime, even if no code changes within your application.&lt;/p&gt; &lt;p&gt;The key elements include secure and up-to-date:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;base container images&lt;/li&gt; &lt;li&gt;Node.js runtime&lt;/li&gt; &lt;li&gt;dependencies&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Based on the team's experience, here are some recommended tips:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Take advantage of container images that come with Node.js already bundled in. The maintainers usually release an update after fixing a &lt;a href="https://www.cve.org"&gt;CVE&lt;/a&gt; reported against the Node.js runtime or any other components within the container. This is one of the reasons the team members often use the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/nodejs-16/615aee9fc739c0a4123a87e1"&gt;ubi/nodejs&lt;/a&gt; container images.&lt;/li&gt; &lt;li&gt;If you build Node.js binaries into a base image, subscribe to and read the &lt;a href="https://groups.google.com/g/nodejs-sec"&gt;nodejs-sec&lt;/a&gt; mailing list. This low-volume mailing list provides advance notice of security releases and will give you the earliest warning to update your Node.js version.&lt;/li&gt; &lt;li&gt;If you use common dependencies across many projects, create a dependency image from which each project reads. While this centralization is suitable for build times, as outlined in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/building-good-containers.md#dependency-image"&gt;dependency image&lt;/a&gt; section of the reference architecture, it also helps reduce the total work required for dependency updates when shared across numerous projects.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For a more exhaustive list of tips, check out the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/development/secure-development-process.md"&gt;Secure Development Process&lt;/a&gt; section of the reference architecture.&lt;/p&gt; &lt;h2&gt;8. Maintaining individual modules&lt;/h2&gt; &lt;p&gt;When you maintain modules in GitHub, enable &lt;a href="https://docs.snyk.io/integrations/git-repository-scm-integrations/github-integration"&gt;Snyk integration&lt;/a&gt; and review the pull requests it creates.&lt;/p&gt; &lt;p&gt;It is also important to test and ensure the module runs and passes tests on the latest Long Term Support (LTS) version of Node.js. Automated testing reduces risk when Node.js security releases require updates.&lt;/p&gt; &lt;h2&gt;Coming next&lt;/h2&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;Node.js reference architecture series&lt;/a&gt;. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;Node.js reference architecture repository&lt;/a&gt; on GitHub, where you will see the work we have done and look forward to future topics.&lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="/topics/nodejs"&gt;Node.js page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="/articles/2022/08/09/8-elements-securing-nodejs-applications" title="8 elements of securing Node.js applications"&gt;8 elements of securing Node.js applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br&gt;&lt;br&gt;</summary><dc:creator>Lucas Holmquist</dc:creator><dc:date>2022-08-09T07:00:00Z</dc:date></entry><entry><title type="html">Eclipse Vert.x 4.3.3 released!</title><link rel="alternate" href="https://vertx.io/blog/eclipse-vert-x-4-3-3" /><author><name>Julien Viet</name></author><id>https://vertx.io/blog/eclipse-vert-x-4-3-3</id><updated>2022-08-09T00:00:00Z</updated><content type="html">Eclipse Vert.x version 4.3.3 has just been released. It fixes quite a few bugs that have been reported by the community and provides a couple of features. In addition it provides support for virtual threads incubation project.</content><dc:creator>Julien Viet</dc:creator></entry></feed>
